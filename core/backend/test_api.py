#!/usr/bin/env python3
"""
APIæµ‹è¯•è„šæœ¬ - æµ‹è¯•è®­ç»ƒå’Œæ¨ç†èµ„æºé¢„ä¼°API
"""

import asyncio
import json
from typing import Dict, Any

import httpx

API_BASE_URL = "http://localhost:8787/api/v1"


async def test_health_check():
    """æµ‹è¯•å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
    print("ğŸ” æµ‹è¯•å¥åº·æ£€æŸ¥...")
    async with httpx.AsyncClient() as client:
        try:
            response = await client.get(f"{API_BASE_URL.replace('/api/v1', '')}/health")
            if response.status_code == 200:
                print("âœ… å¥åº·æ£€æŸ¥é€šè¿‡")
                return True
            else:
                print(f"âŒ å¥åº·æ£€æŸ¥å¤±è´¥: {response.status_code}")
                return False
        except Exception as e:
            print(f"âŒ å¥åº·æ£€æŸ¥è¿æ¥å¤±è´¥: {e}")
            return False





async def test_training_estimate():
    """æµ‹è¯•è®­ç»ƒèµ„æºé¢„ä¼°API"""
    print("\nğŸ” æµ‹è¯•è®­ç»ƒèµ„æºé¢„ä¼°API...")
    
    # æµ‹è¯•è¯·æ±‚æ•°æ®
    training_request = {
        "model_id": "llama-7b",
        "training_method": "lora",
        "precision": "fp16",
        "batch_size": 8,
        "sequence_length": 2048,
        "gradient_accumulation_steps": 4,
        "optimizer": "adamw",
        "learning_rate": 2e-4,
        "weight_decay": 0.01,
        "data_parallel": 1,
        "tensor_parallel": 1,
        "pipeline_parallel": 1,
        "gradient_checkpointing": True
    }
    
    async with httpx.AsyncClient() as client:
        try:
            response = await client.post(
                f"{API_BASE_URL}/training/estimate",
                json=training_request,
                timeout=30.0
            )
            
            if response.status_code == 200:
                result = response.json()
                print("âœ… è®­ç»ƒèµ„æºé¢„ä¼°æˆåŠŸ")
                print(f"  æ€»æ˜¾å­˜éœ€æ±‚: {result['total_memory_gb']:.2f} GB")
                print(f"  æ¨¡å‹æƒé‡: {result['model_memory_gb']:.2f} GB")
                print(f"  æ¿€æ´»å€¼: {result['activation_memory_gb']:.2f} GB")
                print(f"  ä¼˜åŒ–å™¨çŠ¶æ€: {result.get('optimizer_memory_gb', 0):.2f} GB")
                print(f"  æ¨èGPUæ•°é‡: {result['min_gpu_count']}-{result['optimal_gpu_count']}")
                print(f"  é¢„ä¼°é€Ÿåº¦: {result.get('estimated_tokens_per_second', 0):.0f} tokens/s")
                
                if result.get('recommended_gpus'):
                    print("  æ¨èGPUé…ç½®:")
                    for i, gpu in enumerate(result['recommended_gpus'][:3]):
                        print(f"    {i+1}. {gpu['name']} ({gpu['memory_gb']}GB)")
                
                return result
            else:
                print(f"âŒ è®­ç»ƒèµ„æºé¢„ä¼°å¤±è´¥: {response.status_code}")
                print(f"   å“åº”: {response.text}")
                return None
        except Exception as e:
            print(f"âŒ è®­ç»ƒAPIæµ‹è¯•å¤±è´¥: {e}")
            return None


async def test_inference_estimate():
    """æµ‹è¯•æ¨ç†èµ„æºé¢„ä¼°API"""
    print("\nğŸ” æµ‹è¯•æ¨ç†èµ„æºé¢„ä¼°API...")
    
    # æµ‹è¯•è¯·æ±‚æ•°æ®
    inference_request = {
        "model_id": "llama-7b",
        "backend": "vllm",
        "precision": "fp16",
        "quantization": "none",
        "max_batch_size": 32,
        "max_sequence_length": 2048,
        "max_new_tokens": 1024,
        "tensor_parallel": 1,
        "pipeline_parallel": 1
    }
    
    async with httpx.AsyncClient() as client:
        try:
            response = await client.post(
                f"{API_BASE_URL}/inference/estimate",
                json=inference_request,
                timeout=30.0
            )
            
            if response.status_code == 200:
                result = response.json()
                print("âœ… æ¨ç†èµ„æºé¢„ä¼°æˆåŠŸ")
                print(f"  æ€»æ˜¾å­˜éœ€æ±‚: {result['total_memory_gb']:.2f} GB")
                print(f"  æ¨¡å‹æƒé‡: {result['model_memory_gb']:.2f} GB")
                print(f"  KV Cache: {result['kv_cache_memory_gb']:.2f} GB")
                print(f"  æœ€å¤§å¹¶å‘è¯·æ±‚: {result['max_concurrent_requests']}")
                print(f"  é¢„ä¼°ååé‡: {result['estimated_throughput']:.0f} tokens/s")
                print(f"  é¢„ä¼°å»¶è¿Ÿ P50: {result['estimated_latency_p50_ms']:.1f} ms")
                print(f"  é¢„ä¼°å»¶è¿Ÿ P99: {result['estimated_latency_p99_ms']:.1f} ms")
                
                if result.get('recommended_gpus'):
                    print("  æ¨èGPUé…ç½®:")
                    for i, gpu in enumerate(result['recommended_gpus'][:3]):
                        print(f"    {i+1}. {gpu['name']} ({gpu['memory_gb']}GB)")
                
                return result
            else:
                print(f"âŒ æ¨ç†èµ„æºé¢„ä¼°å¤±è´¥: {response.status_code}")
                print(f"   å“åº”: {response.text}")
                return None
        except Exception as e:
            print(f"âŒ æ¨ç†APIæµ‹è¯•å¤±è´¥: {e}")
            return None


async def test_training_configs():
    """æµ‹è¯•è®­ç»ƒé…ç½®API"""
    print("\nğŸ” æµ‹è¯•è®­ç»ƒé…ç½®API...")
    async with httpx.AsyncClient() as client:
        try:
            response = await client.get(f"{API_BASE_URL}/training/configs")
            if response.status_code == 200:
                configs = response.json()
                print("âœ… è·å–è®­ç»ƒé…ç½®æˆåŠŸ")
                print(f"  è®­ç»ƒæ–¹æ³•: {len(configs.get('training_methods', []))} ç§")
                print(f"  ç²¾åº¦ç±»å‹: {len(configs.get('precision_types', []))} ç§")
                print(f"  ä¼˜åŒ–å™¨: {len(configs.get('optimizers', []))} ç§")
                return configs
            else:
                print(f"âŒ è·å–è®­ç»ƒé…ç½®å¤±è´¥: {response.status_code}")
                return None
        except Exception as e:
            print(f"âŒ è®­ç»ƒé…ç½®APIæµ‹è¯•å¤±è´¥: {e}")
            return None


async def test_inference_backends():
    """æµ‹è¯•æ¨ç†åç«¯API"""
    print("\nğŸ” æµ‹è¯•æ¨ç†åç«¯API...")
    async with httpx.AsyncClient() as client:
        try:
            response = await client.get(f"{API_BASE_URL}/inference/backends")
            if response.status_code == 200:
                backends = response.json()
                print("âœ… è·å–æ¨ç†åç«¯æˆåŠŸ")
                print(f"  æ¨ç†åç«¯: {len(backends.get('backends', []))} ç§")
                print(f"  é‡åŒ–æ–¹æ³•: {len(backends.get('quantization_methods', []))} ç§")
                return backends
            else:
                print(f"âŒ è·å–æ¨ç†åç«¯å¤±è´¥: {response.status_code}")
                return None
        except Exception as e:
            print(f"âŒ æ¨ç†åç«¯APIæµ‹è¯•å¤±è´¥: {e}")
            return None


async def run_comprehensive_test():
    """è¿è¡Œç»¼åˆæµ‹è¯•"""
    print("ğŸš€ å¼€å§‹APIç»¼åˆæµ‹è¯•...\n")
    
    # æµ‹è¯•å¥åº·æ£€æŸ¥
    health_ok = await test_health_check()
    if not health_ok:
        print("\nâŒ æœåŠ¡å™¨æœªå¯åŠ¨æˆ–æ— æ³•è¿æ¥ï¼Œè¯·å…ˆå¯åŠ¨åç«¯æœåŠ¡å™¨")
        print("   è¿è¡Œå‘½ä»¤: cd core/backend && python run_server.py")
        return
    
    # æµ‹è¯•å„ä¸ªAPIç«¯ç‚¹
    training_configs = await test_training_configs()
    inference_backends = await test_inference_backends()
    training_result = await test_training_estimate()
    inference_result = await test_inference_estimate()
    
    # æµ‹è¯•ç»“æœç»Ÿè®¡
    print("\nğŸ“Š æµ‹è¯•ç»“æœç»Ÿè®¡:")
    tests = [
        ("å¥åº·æ£€æŸ¥", health_ok),
        ("è®­ç»ƒé…ç½®API", training_configs is not None),
        ("æ¨ç†åç«¯API", inference_backends is not None),
        ("è®­ç»ƒé¢„ä¼°API", training_result is not None),
        ("æ¨ç†é¢„ä¼°API", inference_result is not None)
    ]
    
    passed = sum(1 for _, result in tests if result)
    total = len(tests)
    
    for test_name, result in tests:
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        print(f"  {test_name}: {status}")
    
    print(f"\næ€»ä½“ç»“æœ: {passed}/{total} æµ‹è¯•é€šè¿‡")
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰APIæµ‹è¯•é€šè¿‡ï¼åç«¯æœåŠ¡è¿è¡Œæ­£å¸¸ã€‚")
    else:
        print("âš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥åç«¯æœåŠ¡å’ŒAPIå®ç°ã€‚")


if __name__ == "__main__":
    # è¿è¡Œæµ‹è¯•
    asyncio.run(run_comprehensive_test()) 
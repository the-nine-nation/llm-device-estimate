"""
训练资源计算服务
"""

from typing import Dict, Any, Optional
import math

from ...models.training import TrainingRequest, TrainingResponse, TrainingMethod, OptimizerType, DeepSpeedStage
from ...models.common import PrecisionType, ModelInfo, ModelSize
from ...services.model_registry import ModelRegistry
from .base_calc import BaseCalculator


class TrainingCalculator(BaseCalculator):
    """训练资源计算器"""
    
    # 优化器状态系数
    OPTIMIZER_STATE_MULTIPLIER = {
        OptimizerType.ADAMW: 2.0,  # beta1, beta2 动量
        OptimizerType.ADAM: 2.0,   # beta1, beta2 动量  
        OptimizerType.SGD: 1.0     # 动量
    }
    
    # LoRA参数比例估算
    LORA_PARAM_RATIO = {
        TrainingMethod.LORA: 0.01,      # 约1%的参数
        TrainingMethod.QLORA: 0.01,     # 约1%的参数
        TrainingMethod.ADALORA: 0.015,  # 约1.5%的参数
        TrainingMethod.DORA: 0.02       # 约2%的参数
    }
    
    def __init__(self):
        """初始化训练计算器"""
        super().__init__()
        self.model_registry = ModelRegistry()
    
    def calculate(self, request: TrainingRequest) -> TrainingResponse:
        """
        计算训练资源需求
        
        Args:
            request: 训练预估请求
            
        Returns:
            训练资源预估结果
        """
        # 获取模型信息
        model = self._get_model_info(request)
        
        # 计算各种内存需求
        model_memory = self._calculate_model_memory(model, request)
        activation_memory = self._calculate_activation_memory(model, request)
        optimizer_memory = self._calculate_optimizer_memory(model, request)
        gradient_memory = self._calculate_gradient_memory(model, request)
        
        # 计算总内存
        total_memory = (model_memory + activation_memory + 
                       (optimizer_memory or 0) + (gradient_memory or 0) +
                       self.get_framework_overhead("pytorch"))
        
        # 计算有效批次大小
        effective_batch_size = (request.batch_size * 
                              request.gradient_accumulation_steps * 
                              request.data_parallel)
        
        # 计算GPU需求
        memory_per_gpu = total_memory / request.data_parallel
        min_gpu_count = max(1, math.ceil(total_memory / 80))  # 假设80GB显存
        optimal_gpu_count = request.data_parallel * request.tensor_parallel
        
        # 内存分解
        memory_breakdown = {
            "model_weights": model_memory,
            "activations": activation_memory,
            "optimizer_states": optimizer_memory or 0,
            "gradients": gradient_memory or 0,
            "framework_overhead": self.get_framework_overhead("pytorch")
        }
        
        # 性能预估
        estimated_tokens_per_second = self._estimate_training_speed(model, request)
        
        # 生成推荐GPU列表
        from ...utils.helpers import recommend_gpus
        recommended_gpus = recommend_gpus(memory_per_gpu, max_count=5, use_case="training")
        
        # 生成优化建议
        recommendations = self._generate_recommendations(model, request, total_memory)
        
        return TrainingResponse(
            # 基础内存信息
            total_memory_gb=total_memory,
            model_memory_gb=model_memory,
            activation_memory_gb=activation_memory,
            optimizer_memory_gb=optimizer_memory,
            gradient_memory_gb=gradient_memory,
            framework_overhead_gb=self.get_framework_overhead("pytorch"),
            recommended_gpus=recommended_gpus,
            min_gpu_count=min_gpu_count,
            optimal_gpu_count=optimal_gpu_count,
            
            # 训练特定信息
            training_method=request.training_method,
            effective_batch_size=effective_batch_size,
            memory_per_gpu=memory_per_gpu,
            memory_breakdown=memory_breakdown,
            estimated_tokens_per_second=estimated_tokens_per_second,
            estimated_time_per_epoch=None,  # 需要更多信息才能计算
            recommendations=recommendations
        )
    
    def _get_model_info(self, request: TrainingRequest) -> ModelInfo:
        """获取模型信息"""
        if request.model_id:
            return self.model_registry.get_model_info(request.model_id)
        elif request.custom_model:
            return request.custom_model
        elif request.parameters_billion:
            # 根据参数数量创建临时模型信息
            return self._create_model_from_parameters(request.parameters_billion)
        else:
            raise ValueError("必须提供模型信息")
    
    def _create_model_from_parameters(self, parameters_billion: float) -> ModelInfo:
        """根据参数数量创建模型信息"""
        # 将参数转换为实际数量
        parameters = int(parameters_billion * 1e9)
        
        # 根据参数数量估算模型架构（基于Transformer架构的经验公式）
        # 这些公式基于主流大语言模型的架构规律
        
        if parameters_billion <= 1:
            # 小于1B的模型
            hidden_size = 1024
            num_layers = 12
            num_heads = 16
        elif parameters_billion <= 3:
            # 1-3B模型
            hidden_size = 2048
            num_layers = 24
            num_heads = 16
        elif parameters_billion <= 7:
            # 3-7B模型
            hidden_size = 4096
            num_layers = 32
            num_heads = 32
        elif parameters_billion <= 13:
            # 7-13B模型
            hidden_size = 5120
            num_layers = 40
            num_heads = 40
        elif parameters_billion <= 30:
            # 13-30B模型
            hidden_size = 6656
            num_layers = 60
            num_heads = 52
        elif parameters_billion <= 70:
            # 30-70B模型
            hidden_size = 8192
            num_layers = 80
            num_heads = 64
        else:
            # 70B以上模型
            hidden_size = 12288
            num_layers = 96
            num_heads = 96
        
        return ModelInfo(
            id=f"custom-{parameters_billion}b",
            name=f"自定义 {parameters_billion}B 参数模型",
            family="custom",
            parameters=parameters,
            hidden_size=hidden_size,
            num_layers=num_layers,
            num_heads=num_heads,
            vocab_size=32000,  # 标准词汇表大小
            context_length=2048,  # 默认上下文长度
            architecture="transformer",
            precision=PrecisionType.FP16,
            size_category=self._get_model_size_category(parameters_billion)
        )
    
    def _get_model_size_category(self, parameters_billion: float):
        """根据参数数量确定模型规模类别"""
        if parameters_billion < 1:
            return ModelSize.SMALL
        elif parameters_billion < 10:
            return ModelSize.MEDIUM
        elif parameters_billion < 100:
            return ModelSize.LARGE
        else:
            return ModelSize.XLARGE
    
    def _calculate_model_memory(self, model: ModelInfo, request: TrainingRequest) -> float:
        """计算模型权重内存"""
        if request.training_method == TrainingMethod.FULL_FINETUNING:
            return self.calculate_model_memory(model, request.precision)
        else:
            # LoRA类方法只需要原模型 + LoRA参数
            base_memory = self.calculate_model_memory(model, request.precision)
            lora_ratio = self.LORA_PARAM_RATIO.get(request.training_method, 0.01)
            lora_memory = base_memory * lora_ratio
            return base_memory + lora_memory
    
    def _calculate_activation_memory(self, model: ModelInfo, request: TrainingRequest) -> float:
        """计算激活值内存"""
        # 激活值大小 = batch_size * sequence_length * hidden_size * num_layers * bytes_per_element
        bytes_per_element = self.PRECISION_BYTES[request.precision]
        
        # 基础激活值
        activation_size = (request.batch_size * request.sequence_length * 
                         model.hidden_size * model.num_layers * bytes_per_element)
        
        # 注意力机制的激活值（Q, K, V）
        attention_size = (request.batch_size * model.num_heads * 
                        request.sequence_length * request.sequence_length * 
                        bytes_per_element * model.num_layers)
        
        total_bytes = activation_size + attention_size
        
        # 梯度检查点可以减少激活值内存
        if request.gradient_checkpointing:
            total_bytes *= 0.3  # 约减少70%
        
        return self.convert_bytes(total_bytes)
    
    def _calculate_optimizer_memory(self, model: ModelInfo, request: TrainingRequest) -> Optional[float]:
        """计算优化器状态内存"""
        if request.training_method != TrainingMethod.FULL_FINETUNING:
            # LoRA方法只优化部分参数
            trainable_params = model.parameters * self.LORA_PARAM_RATIO.get(request.training_method, 0.01)
        else:
            trainable_params = model.parameters
        
        multiplier = self.OPTIMIZER_STATE_MULTIPLIER.get(request.optimizer, 1.0)
        bytes_per_param = self.PRECISION_BYTES[request.precision]
        
        # DeepSpeed ZeRO可以分片优化器状态
        if request.deepspeed_stage in [DeepSpeedStage.STAGE1, DeepSpeedStage.STAGE2, DeepSpeedStage.STAGE3]:
            # 优化器状态在多个GPU间分片
            sharding_factor = request.data_parallel
        else:
            sharding_factor = 1
        
        total_bytes = trainable_params * bytes_per_param * multiplier / sharding_factor
        return self.convert_bytes(total_bytes)
    
    def _calculate_gradient_memory(self, model: ModelInfo, request: TrainingRequest) -> Optional[float]:
        """计算梯度内存"""
        if request.training_method != TrainingMethod.FULL_FINETUNING:
            # LoRA方法只需要部分梯度
            trainable_params = model.parameters * self.LORA_PARAM_RATIO.get(request.training_method, 0.01)
        else:
            trainable_params = model.parameters
        
        bytes_per_param = self.PRECISION_BYTES[request.precision]
        
        # DeepSpeed ZeRO Stage 2/3可以分片梯度
        if request.deepspeed_stage in [DeepSpeedStage.STAGE2, DeepSpeedStage.STAGE3]:
            sharding_factor = request.data_parallel
        else:
            sharding_factor = 1
        
        total_bytes = trainable_params * bytes_per_param / sharding_factor
        return self.convert_bytes(total_bytes)
    
    def _estimate_training_speed(self, model: ModelInfo, request: TrainingRequest) -> Optional[float]:
        """预估训练速度（tokens/s）"""
        # 基础速度（假设A100 GPU）
        base_speed = 2000
        
        # 根据模型大小调整
        if model.parameters > 70e9:  # > 70B
            base_speed *= 0.05
        elif model.parameters > 30e9:  # > 30B
            base_speed *= 0.15
        elif model.parameters > 13e9:  # > 13B
            base_speed *= 0.3
        elif model.parameters > 7e9:   # > 7B
            base_speed *= 0.5
        else:  # <= 7B
            base_speed *= 0.8
            
        # 根据训练方法调整
        if request.training_method != TrainingMethod.FULL_FINETUNING:
            base_speed *= 1.5  # LoRA类方法更快
            
        # 根据序列长度调整
        seq_factor = min(2048, request.sequence_length) / 2048
        base_speed *= seq_factor
        
        # 根据批次大小调整
        batch_factor = min(32, request.batch_size) / 8
        base_speed *= batch_factor
        
        # 根据并行度调整
        parallel_efficiency = 0.9 ** (request.data_parallel - 1)
        base_speed *= parallel_efficiency * request.data_parallel
        
        return max(10, base_speed)  # 最小10 tokens/s
    
    def _generate_recommendations(self, model: ModelInfo, request: TrainingRequest, 
                                total_memory: float) -> Dict[str, Any]:
        """生成优化建议"""
        recommendations = {}
        
        # 内存优化建议
        if total_memory > 80:  # 超过单卡显存
            recommendations["memory_optimization"] = [
                "考虑使用DeepSpeed ZeRO Stage 3进行模型分片",
                "启用gradient_checkpointing减少激活值内存",
                "考虑使用LoRA等参数高效微调方法"
            ]
        
        # 训练方法建议
        if request.training_method == TrainingMethod.FULL_FINETUNING and model.parameters > 10e9:
            recommendations["training_method"] = [
                "对于大模型，建议使用LoRA或QLoRA以减少内存占用",
                "全参数微调可能需要大量GPU资源"
            ]
        
        # 批次大小建议
        if request.batch_size > 32:
            recommendations["batch_size"] = [
                "考虑减少batch_size并增加gradient_accumulation_steps",
                "大批次可能导致显存不足"
            ]
        
        return recommendations 